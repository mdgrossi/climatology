{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ecb85b",
   "metadata": {},
   "source": [
    "---\n",
    "title: Downloading NOAA CO-OPS Data\n",
    "subtitle: Part 1 of 3\n",
    "format:\n",
    "  html:\n",
    "    include-after-body: ../footer.html\n",
    "execute:\n",
    "  enable: false\n",
    "order: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1876eff-e19a-4e28-b10b-07c6a2e4dbbe",
   "metadata": {},
   "source": [
    "This notebook demonstrates downloading atmospheric and water observations from the [National Oceanic and Atmospheric Administration](https://www.noaa.gov) (NOAA) [Center for Operational Oceanographic Products and Services](https://tidesandcurrents.noaa.gov/) (CO-OPS) data portal. The objective is to replicate the [Climatology for Virginia Key, FL](https://bmcnoldy.earth.miami.edu/vk/) page created and maintained by [Brian McNoldy](https://bmcnoldy.earth.miami.edu/) at the [University of Miami](https://welcome.miami.edu) [Rosenstiel School of Marine, Atmospheric, and Earth Science](http://earth.miami.edu).\n",
    "\n",
    "We will retrieve water level, water temperature, and air temperature data from Virginia Key, FL. Ultimately, however, there are several variables that could also be retrieved:\n",
    "\n",
    "- Water level (*i.e.*, tides)\n",
    "- Water temperature\n",
    "- Air temperature\n",
    "- Barometric pressure\n",
    "- Wind speed\n",
    "\n",
    "In this notebook we will download the data, store the metadata, and write these to file. The second notebook, [NOAA-CO-OPS-records](NOAA-CO-OPS-records.ipynb), will filter these data and calculate a set of statistics and records. Part 3, [NOAA-CO-OPS-plots](NOAA-CO-OPS-plots.ipynb), will plot and display the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91869d0-521c-49d9-8806-7c7efb51918d",
   "metadata": {},
   "source": [
    "### Packages and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04210715-0dbd-489c-b68d-857c31a2e7ec",
   "metadata": {},
   "source": [
    "First we import the packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0083e0b1-a8b1-4eb3-8640-d8cf83e41a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from noaa_coops import Station\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20e29a-3ba4-4836-94d7-ebc233ec01d3",
   "metadata": {},
   "source": [
    "By default, Python only displays warnings the first time they are thrown. Ideally, we want a code that does not throw any warnings, but it sometimes takes some trial and error to resolve the issue being warned about. So, for diagnostic purposes, we'll set the kernel to always display warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a29b10-a9ce-4ca6-8ffa-5795d554cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3c19e-41f5-45e5-a980-133f3434fc1d",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473389e-3ab4-4abf-a46a-400235bf3835",
   "metadata": {},
   "source": [
    "Next, we define a number of functions that will come in handy later:\n",
    "\n",
    "- `camel`: This will use the site location to create a directory name in camelCase (*e.g.*, \"virginiaKeyFl\") so that we do not have to do it manually\n",
    "- `get_units`: Quick tool for retrieving the units for a desired variable\n",
    "- `format_date`: Formats a timestamp in YYYYMMDD for noaa_coops utility\n",
    "\n",
    "- `load_atemp`: Fetch air temperature data\n",
    "- `load_water_temp`: Fetch water temperature data\n",
    "- `load_water_level`: Fetch water level (tide) data\n",
    "- `load_hourly_height`: Fetch hourly water level height data, the precurser to the 6-minute water level product\n",
    "\n",
    "- `download_data`: Wrapper function that downloads all of the desired data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65df92-d97f-4424-8abd-0ceae78f02f3",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be76b3e3-7acb-4d9f-ac46-fb9395bcbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel(text):\n",
    "    \"\"\"Convert 'text' to camel case\"\"\"\n",
    "    s = text.replace(',', '').replace('.', '').replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    s = s.split()\n",
    "    if len(text) == 0:\n",
    "        return text\n",
    "    return s[0].lower() + ''.join(i.capitalize() for i in s[1:])\n",
    "\n",
    "def get_units(variable, unit_system):\n",
    "    \"\"\"Return the desired units for 'variable'\"\"\"\n",
    "    deg = u'\\N{DEGREE SIGN}'\n",
    "    unit_options = dict({\n",
    "        'Air Temperature': {'metric': deg+'C', 'english': deg+'F'},\n",
    "        'Barometric Pressure': {'metric': 'mb', 'english': 'mb'},\n",
    "        'Wind Speed': {'metric': 'm/s', 'english': 'kn'},\n",
    "        'Wind Gust': {'metric': 'm/s', 'english': 'kn'},\n",
    "        'Wind Direction': {'metric': 'deg', 'english': 'deg'},\n",
    "        'Water Temperature': {'metric': deg+'C', 'english': deg+'F'},\n",
    "        'Water Level': {'metric': 'm', 'english': 'ft'}\n",
    "    })\n",
    "    return unit_options[variable][unit_system]\n",
    "\n",
    "def format_date(datestr):\n",
    "    \"\"\"Format date strings into YYYYMMDD format\"\"\"\n",
    "    dtdt = pd.to_datetime(datestr)\n",
    "    return dt.strftime(dtdt, '%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a12a3-95c6-401d-878b-9836c2cf239d",
   "metadata": {},
   "source": [
    "#### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791a6c4d-9f93-49d3-a5cc-5d4fedf61437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_atemp(metadata, start_date, end_date, verbose=True):\n",
    "    \"\"\"Download air temperature data from NOAA CO-OPS between 'start_date'\n",
    "    and 'end_date' for 'stationid', 'unit_system', and timezone 'tz'\n",
    "    provided in 'metadata' dictionary.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Retrieving air temperature data')\n",
    "    station = Station(id=metadata['stationid'])\n",
    "    if not start_date:\n",
    "        start_date = format_date(station.data_inventory['Air Temperature']['start_date'])\n",
    "    if not end_date:\n",
    "        end_date = format_date(pd.to_datetime('today') + pd.Timedelta(days=1))\n",
    "    air_temp = station.get_data(\n",
    "        begin_date=start_date,\n",
    "        end_date=end_date,\n",
    "        product='air_temperature',\n",
    "        units=metadata['unit_system'],\n",
    "        time_zone=metadata['tz'])\n",
    "    air_temp.columns = ['atemp', 'atemp_flag']\n",
    "    return air_temp\n",
    "\n",
    "def load_water_temp(metadata, start_date, end_date, verbose=True):\n",
    "    \"\"\"Download water temperature data from NOAA CO-OPS between\n",
    "    'start_date' and 'end_date' for 'stationid', 'unit_system', and\n",
    "    timezone 'tz' provided in 'metadata' dictionary.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Retrieving water temperature data')\n",
    "    station = Station(id=metadata['stationid'])\n",
    "    if not start_date:\n",
    "        start_date = format_date(station.data_inventory['Water Temperature']['start_date'])\n",
    "    if not end_date:\n",
    "        end_date = format_date(pd.to_datetime('today') + pd.Timedelta(days=1))\n",
    "    water_temp = station.get_data(\n",
    "        begin_date=start_date,\n",
    "        end_date=end_date,\n",
    "        product='water_temperature',\n",
    "        units=metadata['unit_system'],\n",
    "        time_zone=metadata['tz'])\n",
    "    water_temp.columns = ['wtemp', 'wtemp_flag']\n",
    "    return water_temp\n",
    "\n",
    "def load_water_level(metadata, start_date, end_date, verbose=True):\n",
    "    \"\"\"Download water level data from NOAA CO-OPS between 'start_date' and\n",
    "    'end_date' for 'stationid', 'unit_system', 'datum', and timezone 'tz'\n",
    "    provided in 'metadata' dictionary.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('Retrieving water level tide data')\n",
    "    station = Station(id=metadata['stationid'])\n",
    "    if not start_date:\n",
    "        start_date = format_date(station.data_inventory['Verified 6-Minute Water Level']['start_date'])\n",
    "    if not end_date:\n",
    "        end_date = format_date(pd.to_datetime('today') + pd.Timedelta(days=1))\n",
    "    water_levels = station.get_data(\n",
    "        begin_date=start_date,\n",
    "        end_date=end_date,\n",
    "        product='water_level',\n",
    "        datum=metadata['datum'],\n",
    "        units=metadata['unit_system'],\n",
    "        time_zone=metadata['tz'])\n",
    "    water_levels.columns = ['wlevel', 's', 'wlevel_flag', 'wlevel_qc']\n",
    "    return water_levels\n",
    "\n",
    "def load_hourly_height(metadata, start_date, end_date, verbose=True):\n",
    "    \"\"\"Download verified hourly height data, the predecessor to the water level product, from NOAA CO-OPS from 'start_date' through 'end_date'.\"\"\"\n",
    "    if verbose:\n",
    "        print('Retrieving hourly height data')\n",
    "    station = Station(id=metadata['stationid'])\n",
    "    hourly_heights = station.get_data(\n",
    "        begin_date=start_date,\n",
    "        end_date=end_date,\n",
    "        product='hourly_height',\n",
    "        datum=metadata['datum'],\n",
    "        units=metadata['unit_system'],\n",
    "        time_zone=metadata['tz'])\n",
    "    hourly_heights.columns = ['wlevel', 's', 'wlevel_flag']\n",
    "    # Add QC column for comparing to water level product\n",
    "    hourly_heights['wlevel_qc'] = 'v'\n",
    "    return hourly_heights\n",
    "\n",
    "def download_data(metadata, start_date=None, end_date=None, verbose=True):\n",
    "    \"\"\"Download data from NOAA CO-OPS\"\"\"\n",
    "    if verbose:\n",
    "        print('Downloading data')\n",
    "    \n",
    "    # NOAA CO-OPS API\n",
    "    station = Station(id=metadata['stationid'])\n",
    "    \n",
    "    # List of data variables to combine at the end\n",
    "    datasets = []\n",
    "            \n",
    "    # If no 'end_date' is passed, download through end of current date\n",
    "    if end_date is None:\n",
    "        end_date = format_date(pd.to_datetime('today') + pd.Timedelta(days=1))\n",
    "    else:\n",
    "        end_date = format_date(end_date)\n",
    "    \n",
    "    # Air temperature\n",
    "    if 'Air Temperature' in station.data_inventory:\n",
    "        if start_date is None:\n",
    "            start_date = format_date(station.data_inventory['Air Temperature']['start_date'])\n",
    "        else:\n",
    "            start_date = format_date(start_date)\n",
    "        air_temp = load_atemp(metadata=metadata, start_date=start_date,\n",
    "                              end_date=end_date, verbose=verbose)\n",
    "        air_temp['atemp_flag'] = air_temp['atemp_flag'].str.split(',', expand=True).astype(int).sum(axis=1)\n",
    "        air_temp.loc[air_temp['atemp_flag'] > 0, 'atemp'] = np.nan\n",
    "        datasets.append(air_temp['atemp'])\n",
    "\n",
    "    # Water temperature\n",
    "    if 'Water Temperature' in station.data_inventory:\n",
    "        if start_date is None:\n",
    "            start_date = format_date(station.data_inventory['Water Temperature']['start_date'])\n",
    "        else:\n",
    "            start_date = format_date(start_date)\n",
    "        water_temp = load_water_temp(metadata=metadata, start_date=start_date,\n",
    "                                     end_date=end_date, verbose=verbose)\n",
    "        water_temp['wtemp_flag'] = water_temp['wtemp_flag'].str.split(',', expand=True).astype(int).sum(axis=1)\n",
    "        water_temp.loc[water_temp['wtemp_flag'] > 0, 'wtemp'] = np.nan\n",
    "        datasets.append(water_temp['wtemp'])\n",
    "\n",
    "    # Water level (tides)\n",
    "    if 'Verified 6-Minute Water Level' in station.data_inventory:\n",
    "        if start_date is None:\n",
    "            start_date = format_date(station.data_inventory['Verified 6-Minute Water Level']['start_date'])\n",
    "        else:\n",
    "            start_date = format_date(start_date)\n",
    "        water_levels = load_water_level(metadata=metadata, start_date=start_date,\n",
    "                                        end_date=end_date, verbose=verbose)\n",
    "        water_levels['wlevel_flag'] = water_levels['wlevel_flag'].str.split(',', expand=True).astype(int).sum(axis=1)\n",
    "        water_levels.loc[water_levels['wlevel_flag'] > 0, 'wlevel'] = np.nan\n",
    "        \n",
    "        # Hourly water heights (historical product)\n",
    "        if start_date is None:\n",
    "            if 'Verified Hourly Height Water Level' in station.data_inventory:\n",
    "                start = format_date(station.data_inventory['Verified Hourly Height Water Level']['start_date'])\n",
    "                end = format_date(water_levels.index[0] + pd.Timedelta(days=1))\n",
    "                hourly_heights = load_hourly_height(metadata=metadata, \n",
    "                                                    start_date=start, end_date=end, verbose=verbose)\n",
    "                hourly_heights['wlevel_flag'] = \\\n",
    "                        hourly_heights['wlevel_flag'].str\\\n",
    "                            .split(',', expand=True).astype(int).sum(axis=1)\n",
    "                hourly_heights.loc[hourly_heights['wlevel_flag'] > 0] = np.nan\n",
    "                water_levels = pd.concat(\n",
    "                    (hourly_heights[['wlevel', 'wlevel_flag', 'wlevel_qc']][:-1], \n",
    "                    water_levels[['wlevel', 'wlevel_flag', 'wlevel_qc']]), axis=0\n",
    "                )\n",
    "                water_levels = water_levels[~water_levels.index.duplicated(keep='first')]\n",
    "        datasets.append(water_levels[['wlevel', 'wlevel_qc']])\n",
    "\n",
    "    # Merge into single dataframe and rename columns\n",
    "    if verbose:\n",
    "        print('Compiling data')\n",
    "    newdata = pd.concat(datasets, axis=1)\n",
    "    newdata.index.name = f'time_{metadata[\"tz\"]}'\n",
    "    newdata.columns = [i for i in metadata['variables']+['Water Level QC']]\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db6640-a7e4-4549-b3ee-849b117715bf",
   "metadata": {},
   "source": [
    "### Load / download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d604ab3f-732a-45e9-b72a-8c8c19fb58a9",
   "metadata": {},
   "source": [
    "Now it's time to load the data. First, specify the station we want to load. This will be used to load saved data or download all data from a new station, if we have not yet retrieved data from this particular `stationname`.\n",
    "\n",
    "`stationname` is a custom human-readable \"City, ST\" string for the station, while `id` is the NOAA-COOPS station ID number.\n",
    "\n",
    "We also need to specify other settings such as `datum`, timezone (`tz`), and unit system.\n",
    "\n",
    "Finally, we indicate `hr_threshold`, the maximum number of hours of missing data allowed before a day is not counted in the records, and `day_threshold`, the maximum number of days of missing data allowed before a month is not counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a528859-8acb-4069-a3d2-9b1ddb29ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationname = 'Virginia Key, FL'\n",
    "id = '8723214'\n",
    "datum = 'MHHW'\n",
    "tz = 'lst'\n",
    "unit_system = 'english'\n",
    "hr_threshold = 4\n",
    "day_threshold = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0d94e-b52d-47e8-bebb-d40b55bd8fab",
   "metadata": {},
   "source": [
    "Derive the directory name containing for data from the station name. This is where the data are or will be saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca70399f-6e73-4944-a89f-eb9e3c51e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station folder: virginiaKeyFl\n",
      "Full directory: ../virginiaKeyFl\n"
     ]
    }
   ],
   "source": [
    "dirname = camel(stationname)\n",
    "outdir = os.path.join(os.getcwd(), dirname)\n",
    "outdir = f'../{dirname}'\n",
    "\n",
    "print(f\"Station folder: {dirname}\")\n",
    "print(f\"Full directory: {outdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99ebe4-4d23-4d85-b8ea-9f5b4be39b52",
   "metadata": {},
   "source": [
    "Flag for printing statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0266df44-d651-4114-9120-eb2e71a4bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d2fc2-64e1-48a4-9571-4e4d93150045",
   "metadata": {},
   "source": [
    "Let's see if we already have data from this station saved locally. This will be true if a directory already exists for the station.\n",
    "\n",
    "If the directory `outdir` does not exist, then no data have been downloaded for this station, so we need to download everything through the present. This requires a few steps:\n",
    "\n",
    "1. Create `outdir`\n",
    "2. Download the data and record the timestamp of the last observation for each variable in the metadata. This will be used later when updating the data.\n",
    "3. Write the data and metadata to file.\n",
    "\n",
    "On the other hand, if data already exist locally, we will load it from file and download new data we do not yet have:\n",
    "\n",
    "1. Load the data and metadata from file\n",
    "2. Retrieve new data\n",
    "3. Combine new data to existing data, update the 'last_obs' metadata entries, and write data and metadata to file\n",
    "\n",
    "The noaa-coops tool only accepts dates without times, so it is possible to download data we already have. We therefore have to check what we download against what we already have to avoid duplicating data.\n",
    "\n",
    "The most likely (and perhaps only) scenerio for encountering duplicated data is if the data we have for the most recent day is incomplete. For example, assume today is May 5, 2025 and we download data at noon. Also assume the start date is some earlier day, the last time we retrieved data, and this will be automatically determined from the metadata. Specifying an end date `2025-05-05` will retrieve all data available through noon on May 5. In this case, we do not yet have these data, so we concatenate what we do not have to what we do have. However, if we then run the download function again (say, for diagnostic purposes) with the new start date of `2025-05-01` and the end date `2025-05-05`, it will again download the data through noon on May 5. But since we already have those data, we do not want to re-concatenate them.\n",
    "\n",
    "*This cell may take several seconds or minutes to run, depending on how much data are downloaded.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77b9030-b586-4a20-85d0-ba71d1bde218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from file\n",
      "Loading historical data from file\n",
      "Downloading data\n",
      "Retrieving air temperature data\n",
      "Retrieving water temperature data\n",
      "Retrieving water level tide data\n",
      "Compiling data\n",
      "Updated observational data written to file 'observational_data_record.csv'.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(outdir):\n",
    "    if verbose:\n",
    "        print('Creating new directory for this station.')\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "    # Save metadata to file\n",
    "    meta = dict({\n",
    "        'stationname': stationname,\n",
    "        'stationid': stationid,\n",
    "        'dirname': dirname,\n",
    "        'unit_system': unit_system,\n",
    "        'tz': tz,\n",
    "        'datum': datum,\n",
    "        'hr_threshold': hr_threshold,\n",
    "        'day_threshold': day_threshold,\n",
    "        'variables': ['Air Temperature', 'Water Temperature', 'Water Level']})\n",
    "    meta['units'] = [get_units(var, meta['unit_system']) for var in meta['variables']]\n",
    "    with open(os.path.join(outdir, 'metadata.yml'), 'w') as fp:\n",
    "        yaml.dump(meta, fp)\n",
    "\n",
    "    # Download all data (set start and end date to None to get all data)\n",
    "    if verbose:\n",
    "        print('Downloading all data for this station.')\n",
    "    data = download_data(metadata=meta, start_date=None, end_date=None)\n",
    "    outFile = os.path.join(outDir, 'observational_data_record.csv.gz')\n",
    "    data.to_csv(outDir, compression='infer')\n",
    "    meta['last_obs'] = {i:data[i].last_valid_index().strftime('%Y-%m-%d %X') \\\n",
    "                        for i in meta['variables']}\n",
    "    print(f\"Observational data written to file '{outFile}'.\")\n",
    "    \n",
    "else:\n",
    "    # Load the metadata\n",
    "    if verbose:\n",
    "        print('Loading metadata from file')\n",
    "    with open(os.path.join(outdir, 'metadata.yml')) as m:\n",
    "        meta = yaml.safe_load(m)\n",
    "    \n",
    "    # Load the historical data\n",
    "    if verbose:\n",
    "        print('Loading historical data from file')\n",
    "    dataInFile = os.path.join(outdir, 'observational_data_record.csv.gz')\n",
    "    dtypeDict = {k: float for k in meta['variables']}\n",
    "    dtypeDict['Water Level QC'] = str\n",
    "    data = pd.read_csv(dataInFile, index_col=f'time_{meta[\"tz\"]}', parse_dates=True,\n",
    "                       compression='infer', dtype=dtypeDict)\n",
    "    start_date = format_date(data.index.max())\n",
    "\n",
    "    # Retrieve new data\n",
    "    newdata = download_data(metadata=meta, start_date=start_date)\n",
    "    if sum(~newdata.index.isin(data.index)) == 0:\n",
    "        print('No new data available.')\n",
    "    else:\n",
    "        newdata.index.name = f\"time_{meta['tz']}\"\n",
    "        newdata.columns = [i for i in data.columns]\n",
    "        data = pd.concat([data,\n",
    "                          newdata[newdata.index.isin(data.index) == False]], axis=0)\n",
    "        data.to_csv(dataInFile, compression='infer')\n",
    "        meta['last_obs'] = {i:data[i].last_valid_index().strftime('%Y-%m-%d %X') \\\n",
    "                            for i in meta['variables']}\n",
    "        print(\"Updated observational data written to file 'observational_data_record.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a4e80-cf5e-44e3-b7fc-e3d8ff343ca7",
   "metadata": {},
   "source": [
    "Check the data and metadata for sanity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b196a01e-3e72-4473-a957-0ce11334a96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air Temperature</th>\n",
       "      <th>Water Temperature</th>\n",
       "      <th>Water Level</th>\n",
       "      <th>Water Level QC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_lst</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1994-01-28 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-28 00:06:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-28 00:12:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-28 00:18:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-28 00:24:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-30 19:36:00</th>\n",
       "      <td>86.7</td>\n",
       "      <td>88.2</td>\n",
       "      <td>-1.657</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-30 19:42:00</th>\n",
       "      <td>86.5</td>\n",
       "      <td>88.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-30 19:48:00</th>\n",
       "      <td>86.5</td>\n",
       "      <td>88.2</td>\n",
       "      <td>-1.552</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-30 19:54:00</th>\n",
       "      <td>86.4</td>\n",
       "      <td>88.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-30 20:00:00</th>\n",
       "      <td>86.4</td>\n",
       "      <td>88.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2745205 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Air Temperature  Water Temperature  Water Level  \\\n",
       "time_lst                                                               \n",
       "1994-01-28 00:00:00              NaN                NaN          NaN   \n",
       "1994-01-28 00:06:00              NaN                NaN          NaN   \n",
       "1994-01-28 00:12:00              NaN                NaN          NaN   \n",
       "1994-01-28 00:18:00              NaN                NaN          NaN   \n",
       "1994-01-28 00:24:00              NaN                NaN          NaN   \n",
       "...                              ...                ...          ...   \n",
       "2025-05-30 19:36:00             86.7               88.2       -1.657   \n",
       "2025-05-30 19:42:00             86.5               88.2          NaN   \n",
       "2025-05-30 19:48:00             86.5               88.2       -1.552   \n",
       "2025-05-30 19:54:00             86.4               88.2          NaN   \n",
       "2025-05-30 20:00:00             86.4               88.2          NaN   \n",
       "\n",
       "                    Water Level QC  \n",
       "time_lst                            \n",
       "1994-01-28 00:00:00            NaN  \n",
       "1994-01-28 00:06:00            NaN  \n",
       "1994-01-28 00:12:00            NaN  \n",
       "1994-01-28 00:18:00            NaN  \n",
       "1994-01-28 00:24:00            NaN  \n",
       "...                            ...  \n",
       "2025-05-30 19:36:00              p  \n",
       "2025-05-30 19:42:00              p  \n",
       "2025-05-30 19:48:00              p  \n",
       "2025-05-30 19:54:00              p  \n",
       "2025-05-30 20:00:00              p  \n",
       "\n",
       "[2745205 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aaa907c-69e8-4434-92a7-e40f63932a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datum': 'MHHW',\n",
       " 'day_threshold': 2,\n",
       " 'dirname': 'virginiaKeyFl',\n",
       " 'hr_threshold': 4,\n",
       " 'stationid': '8723214',\n",
       " 'stationname': 'Virginia Key, FL',\n",
       " 'tz': 'lst',\n",
       " 'unit_system': 'english',\n",
       " 'units': {'Air Temperature': '°F',\n",
       "  'Water Level': 'ft',\n",
       "  'Water Temperature': '°F'},\n",
       " 'variables': ['Air Temperature', 'Water Temperature', 'Water Level'],\n",
       " 'last_obs': {'Air Temperature': '2025-05-30 20:00:00',\n",
       "  'Water Temperature': '2025-05-30 20:00:00',\n",
       "  'Water Level': '2025-05-30 19:48:00'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc50ac9-fa54-47c4-b7f3-4231c7d0092d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.index.unique()) == data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a43a7-6fa4-4800-87f1-794ac5470dc2",
   "metadata": {},
   "source": [
    "The 'last_obs' metadata values matches the last observation in the data record and corresponds to the most recently available observation. Also, every observation time is unique, so there are no duplicated entries. So, everything checks out.\n",
    "\n",
    "[In the next part](NOAA-CO-OPS-records.ipynb), we will filter these data and calculate statistics and records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630e198-4ab8-45e9-a18e-046c2980eaa5",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
